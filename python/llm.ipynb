{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e52e4e6c7c44594a51744af5feb9a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35fe665ad054021a76b9fb20e9623b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, FlaxGemmaForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import time\n",
    "from memo import MEMORY\n",
    "import torch.cuda as tc\n",
    "from flask import Flask, jsonify\n",
    "\n",
    "# Specify the device as 'cuda' for AMD GPU if PyTorch with ROCm maps AMD GPUs to CUDA devices\n",
    "device = 'cuda' if tc.is_available() else 'cpu'\n",
    "\n",
    "# Specify the torch dtype based on AMD GPU support\n",
    "dtype = torch.bfloat16 if device == 'cuda' else torch.float32\n",
    "\n",
    "login(os.getenv('HF_TOKEN'))\n",
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "print(device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joinvc\n",
      "I am unable to interact with external platforms or individuals, including sending messages or inviting participation in activities. My purpose is to provide information and assist users within the Library's boundaries.\n"
     ]
    }
   ],
   "source": [
    "messages = ''\n",
    "prompt = MEMORY\n",
    "while True:\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "\n",
    "            with open('../input.txt') as f:\n",
    "                input = f.read()\n",
    "                f.close()\n",
    "            with open('../talker.txt') as f:\n",
    "                talker = f.read()\n",
    "                f.close()\n",
    "            if 'staple' in talker.lower() or 'stapler_x' in talker.lower():\n",
    "                talker = 'max'\n",
    "            if (input and input != '') and input != None:\n",
    "                messages = input\n",
    "                prompt.append({\"role\": \"user\", \"content\": f'{talker} says {messages}'})\n",
    "                \n",
    "                print(messages)\n",
    "                template = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = tokenizer.encode(template, add_special_tokens=False, return_tensors=\"pt\")\n",
    "                model_response = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "                output = tokenizer.decode(model_response[0], skip_special_tokens=True)\n",
    "                index = output.rfind(\"model\")\n",
    "                chat = output[index+len(\"model\"):].strip()\n",
    "                print(chat)\n",
    "\n",
    "                prompt.append({\"role\": \"model\", \"content\": chat})\n",
    "                with open('../data/input.txt', 'w') as fp:\n",
    "                    pass\n",
    "                with open('../data/output.txt', 'w') as f:\n",
    "                    f.write(chat)\n",
    "                    f.close()\n",
    "                with open('../data/data.txt', 'a') as f:\n",
    "                    f.write(chat)\n",
    "                    f.close()\n",
    "    except Exception as e:\n",
    "        print('Error ' + str(e))\n",
    "        with open('../data/input.txt', 'w') as fp:\n",
    "            pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
